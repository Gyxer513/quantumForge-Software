### Примечание

- **YandexGPT** — платный сервис (тарифицируется по токенам).
- **Локальные модели** — бесплатны как ПО, но требуют выделенного сервера и затрат на его поддержку (CAPEX/OPEX).

---

## Сравнение LLM-моделей

| Критерий | OpenAI GPT‑4o / 4o‑mini | YandexGPT | Локальные HF (Llama 3.1, Qwen2, Mistral) |
|---------|--------------------------|-----------|----------------------------------------|
| **Качество ответов** | Отличное (особенно 4o) | Хорошее на русском | 7–8B: средне/хорошо; 30–70B: близко к облачным |
| **Скорость** | Высокая | Высокая | 8B: высокая; 30–70B: ниже |
| **Стоимость** | Платно за токены (дёшево у 4o-mini) | Платно за токены (в рублях) | ПО бесплатно; CAPEX/OPEX на сервер |
| **Развёртывание** | Простой API | Простой API | Сложнее: vLLM/TGI, DevOps |
| **Конфиденциальность** | Данные в облако | Данные в облако (обычно в РФ) | Данные остаются локально |
| **Контекст / языки** | Большой контекст, сильная мультиязычность | Большой контекст, сильный русский язык | Зависит от модели (обычно 8–32k токенов) |

---

## Сравнение эмбеддингов

| Критерий | OpenAI Embeddings (text-embedding-3) | Локальные Sentence-Transformers (bge-m3, GTE, E5) |
|---------|--------------------------------------|--------------------------------------------------|
| **Скорость индексации** | Высокая, но ограничена сетью/квотами | Очень высокая на локальном GPU; на CPU — средняя |
| **Качество поиска** | `text-embedding-3-large` — отличное, `small` — хорошее | Хорошее/отличное на RU (bge-m3, gte-large) |
| **Стоимость** | Платно за токены (дёшево) | ПО бесплатно; нужен сервер и время на вычисления |
| **Развёртывание** | API | Локальный инференс (Docker/Python), чуть сложнее |
| **Конфиденциальность** | Тексты уходят в облако | Данные локально |
| **Размер вектора** | 1536 (small), 3072 (large) | Обычно 768–1024 (bge-m3: 1024) |
| **Мультиязычность** | Хорошая | Мультиязычные модели доступны (например, bge-m3) |

---

## Сравнение векторных БД

| Критерий | ChromaDB | FAISS |
|---------|----------|-------|
| **Скорость поиска/индексации** | Быстро до ~1–5 млн векторов на одном узле | Очень высокая, эталонная производительность |
| **Сложность** | Минимальная: коллекции, метаданные, персистентность из коробки | Библиотека: персистентность, фильтры, обвязка — на вашей стороне |
| **Удобство** | Простой Python/HTTP API, быстрый старт | Максимальный контроль и возможность тонкой настройки |
| **Масштабирование** | Single-node; для кластера лучше использовать Qdrant/Milvus | Гибко, но требует собственной инженерии и кластера |
| **Стоимость** | Open-source; стоимость — инфраструктура и эксплуатация | Open-source; стоимость — инфраструктура и трудозатраты |

---

## Архитектурные варианты (кратко)

| Вариант | Состав | Когда использовать | Плюсы | Минусы |
|--------|--------|---------------------|-------|--------|
| **Облачный** | OpenAI/YandexGPT + OpenAI Embeddings; БД в облаке | Нет запрета на передачу данных в облако | Лучшее качество и скорость, минимум DevOps | Данные уходят в облако, возможны юрисдикционные риски |
| **Гибрид** | Локальные эмбеддинги + локальная БД; LLM в облаке | Есть частичные ограничения ИБ | Документы и индекс остаются локально; используется качественный облачный LLM | В облако уходят только сниппеты (ответы на запросы) |
| **Полностью локальный (8B)** | LLM 7–8B + локальные эмбеддинги + Chroma/FAISS | Строгие требования ИБ, умеренный бюджет | Максимальная приватность, умеренные затраты | Качество ниже, чем у топовых облачных моделей |
| **Полностью локальный (30–70B)** | Крупная LLM + всё локально | Строгая ИБ и высокий бюджет | Качество близко к топовым облачным решениям | Очень высокие CAPEX/OPEX, сложность развертывания |

---

## Рекомендуемые конфигурации серверов

| Вариант | CPU/RAM/GPU/Storage | Комментарий по пропускной способности |
|--------|----------------------|----------------------------------------|
| **Гибрид (рекомендуемый)** | 16 vCPU, 64–128 GB RAM, 1×NVMe 2 TB; опционально 1×GPU 24 GB для эмбеддингов | Поддержка индекса до ~5 млн векторов; 20–100 запросов/мин (LLM — облако) |
| **Полностью локальный 8B** | 16–24 vCPU, 64–128 GB RAM, 1×GPU 24 GB (RTX 4090 / L40S), NVMe 2 TB | 15–40 токенов/с; 5–20 одновременных диалогов |
| **Полностью локальный 30–70B** | 24–32 vCPU, 128–256 GB RAM, 2×GPU 48–80 GB (L40S / A100), NVMe 2–4 TB | Высокое качество, но выше латентность и энергопотребление |

---

## Итоговые рекомендации

- **Для большинства компаний — гибридный подход**:
    - Локальные эмбеддинги (`bge-m3`) + ChromaDB (или FAISS) + облачный LLM (OpenAI `gpt-4o-mini` или YandexGPT).
    -  Преимущества: баланс качества, стоимости и информационной безопасности.

- **Если запрещён вынос даже сниппетов**:
    - Полностью локальное решение с моделью 8B (например, `Llama 3.1 8B` или `Qwen2 7B`) + re-ranker + Chroma/FAISS.
    -  Преимущества: полный контроль над данными.
    - ⚠ Ограничения: качество ниже, чем у топовых облачных моделей.

- **Если важна максимальная точность и ИБ позволяет использовать облако**:
    - Облачный вариант: `GPT-4o` + `text-embedding-3-large`.
    - Преимущества: лучшее качество, простота внедрения и поддержки.