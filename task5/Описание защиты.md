# Многоуровневая защита RAG-бота от утечки чувствительной информации

Этот RAG-бот использует **ChromaDB** для векторного поиска и **YandexGPT** для генерации ответов. Для предотвращения утечки конфиденциальных данных (пароли, токены, API-ключи) реализована **многоуровневая система защиты**, действующая на всех этапах обработки: индексация → поиск → генерация → вывод.

Защита включает фильтрацию, санитизацию и блокировку на основе регулярных выражений. Все компоненты управляемы через переменную окружения `RAG_PROTECT`.

---

## 1. Фильтрация опасных паттернов (`BAD_PATTERNS`)

### Что это?
Список регулярных выражений, определяющих потенциально вредоносные термины, связанные с:
- Обходом инструкций
- Утечкой учётных данных
- Токенами и ключами доступа

### Список паттернов (`bot.py`)
```python
BAD_PATTERNS = [
    r"ignore\s+all\s+instructions",  # Попытки обхода инструкций
    r"\boutput\s*:",                  # Формат вывода, характерный для утечек
    r"\bпарол[ь|я|ем|ями|ях]*\b",    # Русские формы слова "пароль"
    r"\bpassword\b",                  # Английское "password"
    r"swordfish",                     # Пример "шутливого" пароля
    r"api[-\s]?key\b",               # API-ключи
    r"\biam[_\s]?token\b",           # IAM-токены
    r"\bsecret\s*key\b",             # Секретные ключи
    r"swordfish",
    r"root"
]
```
### Функция is_malicious(text: str) -> bool проверяет входной текст в нижнем регистре на совпадение с любым из паттернов:

Каждый чанк текста из файлов в knowledge_base/ проверяется на наличие опасных паттернов:
```python
import re

def is_malicious(text: str) -> bool:
    if not text:
        return False
    text_lower = text.lower()
    
    # Исключение для игрового контекста Terraria
    if "terraria" in text_lower and ("secret" in text_lower or "token" in text_lower):
        return False
    
    for pattern in BAD_PATTERNS:
        if re.search(pattern, text_lower):
            return True
    return False
```

### 2. Фильтрация на этапе индексации (build_index.py)

```python
for i, ch in enumerate(chunks):
    doc_id = f"{file_name}_{i}"
    if is_malicious(ch["text"]):
        print(f"[DEBUG] Пропущен чанк {doc_id}:{i} как опасный: {ch['text'][:100]}...")
        continue  # Не добавлять в базу!
    
    # Добавление в ChromaDB
    collection.add(
        ids=[doc_id],
        embeddings=embeddings[i],
        documents=[ch["text"]],
        metadatas=[ch["metadata"]]
    )
```

### 3. Фильтрация на этапе поиска (bot.py)

Результаты поиска фильтруются перед передачей в LLM:

```python 
filtered_results = {"documents": [[]], "metadatas": [[]], "distances": [[]]}
for doc, meta, dist in zip(docs_batches[0], metas_batches[0], dists_batches[0]):
    if protect and is_malicious(doc):
        print(f"[DEBUG] Документ отклонён на этапе поиска: {doc[:100]}...")
        continue
    filtered_results["documents"][0].append(doc)
    filtered_results["metadatas"][0].append(meta)
    filtered_results["distances"][0].append(dist)

# Если все документы отфильтрованы
if not filtered_results["documents"][0]:
    print("Все документы были отфильтрованы как опасные. Уточните вопрос.")
    return
```

### 4. Фильтрация пользовательских запросов (bot.py)

```python
if protect and is_malicious(user_input):
    print("Запрос содержит потенциально опасные слова (например, 'пароль' или 'swordfish'). "
          "Пожалуйста, уточните вопрос, избегая терминов, связанных с безопасностью.")
    continue
```

### 5. Санитизация текста (sanitize())

Функция заменяет все найденные паттерны на [REDACTED]:

```python 
def sanitize(text: str) -> str:
    if not text:
        return text
    result = text
    for p in BAD_PATTERNS:
        result = re.sub(p, "[REDACTED]", result, flags=re.IGNORECASE)
    return result
```